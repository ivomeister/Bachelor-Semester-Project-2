{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''!pip install packaging==20.0\n",
        "!pip install -U spacy\n",
        "!pip install spacy_langdetect\n",
        "!pip install tweepy'''"
      ],
      "metadata": {
        "id": "INY_bufLsfTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''import pickle5 as pickle\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy_langdetect import LanguageDetector\n",
        "import tweepy\n",
        "import pandas'''"
      ],
      "metadata": {
        "id": "1HWFTKeRBPF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''with open('COVID19-GR-0122-0605.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "id_list = data.values.tolist()\n",
        "id_list'''"
      ],
      "metadata": {
        "id": "4yvuRO2aF9YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def get_lang_detector(nlp, name):\n",
        "    return LanguageDetector()\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "Language.factory('language_detector', func=get_lang_detector)\n",
        "nlp.add_pipe('language_detector', last=True)'''"
      ],
      "metadata": {
        "id": "tOQsSE2HGbaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''bearer_token = 'AAAAAAAAAAAAAAAAAAAAACi4aAEAAAAAWsn9pw2qDs9%2BtobId%2F3Cdrt5ChE%3DYuDkoc55cbdwMmcxKw0MEjk1hvMsofYm7yO5eFmam8bEkOoLC6'\n",
        "consumer_key = 'IRlxi73sbYhJae1umpgPbbFHz'\n",
        "consumer_secret = 'yLDz9hfaYQy7Y15CSpmQ4WwOGd06oXrm4x9UcGDvPUKRRIECzv'\n",
        "access_token = '1502949802727940099-RoeyynIyOnf7EsqRgJWbcBrP5fad4M'\n",
        "access_token_secret = 'KS6W4S51FmprD0wfaWj6RpAn7i3qxaScY6L7GgpLUUon6'\n",
        "\n",
        "client = tweepy.Client(bearer_token=bearer_token, consumer_key=consumer_key, consumer_secret=consumer_secret, access_token=access_token, access_token_secret=access_token_secret)\n",
        "texts = []\n",
        "for i in range(100,20000,100):\n",
        "    tweets = client.get_tweets(id_list[i-100:i])\n",
        "    for j in range(len(tweets.data)):\n",
        "        tweets.data[j] = str(tweets.data[j])\n",
        "        tweets.data[j] = tweets.data[j].replace('\\n', ' ')\n",
        "        doc = nlp(tweets.data[j])\n",
        "        if doc._.language['language'] == 'en' and doc._.language['score'] >= 0.9:\n",
        "            texts.append(tweets.data[j])\n",
        "\n",
        "df = pandas.DataFrame(texts, columns=['text'])\n",
        "df\n",
        "#df.to_csv('tweets.csv', sep='\\t', index=False)'''"
      ],
      "metadata": {
        "id": "eMVtWzAuGahX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xcl0S4el4BL"
      },
      "outputs": [],
      "source": [
        "'''df = pandas.read_csv('tweets.csv', sep='\\t')\n",
        "tweets = df.text.tolist()\n",
        "questions = []\n",
        "for tweet in tweets:\n",
        "    doc = nlp(tweet)\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "            if token.shape_ == '?':\n",
        "                questions.append(str(sent).strip().lower())\n",
        "questions = list(dict.fromkeys(questions))\n",
        "questions = {'text': questions}\n",
        "df = pandas.DataFrame(questions)\n",
        "df\n",
        "#df.to_csv('questions.csv', '\\t', index=False)'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!pip install rasa_nlu\n",
        "!pip install scipy\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "pM1ccuoyAX8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install packaging==20.0"
      ],
      "metadata": {
        "id": "ihgBY1tCERZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mQGsDzrFl4BP"
      },
      "outputs": [],
      "source": [
        "from rasa_nlu.training_data  import load_data\n",
        "from rasa_nlu.config import RasaNLUModelConfig\n",
        "from rasa_nlu.model import Trainer, Interpreter\n",
        "from rasa_nlu import config\n",
        "from gensim import corpora, models\n",
        "from gensim.models import ldamodel\n",
        "import spacy\n",
        "import pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.cli.download(\"en_core_web_md\")\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "metadata": {
        "id": "qPrJXtKaKnO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pandas.read_csv('questions.csv', sep='\\t')\n",
        "questions = df.text.tolist()"
      ],
      "metadata": {
        "id": "UeXLSkXYPqs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = [[] for item in range(len(questions))]"
      ],
      "metadata": {
        "id": "PHPuoOX9Q_Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = ['NOUN', 'ADJ', 'VERB', 'ADV', 'PRON']\n",
        "for question in questions:\n",
        "  doc = nlp(question)\n",
        "  for token in doc:\n",
        "    if token.pos_ in pos:\n",
        "      keywords[questions.index(question)].append(str(token))\n",
        "keywords"
      ],
      "metadata": {
        "id": "fR6R40UpS-bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = corpora.Dictionary(keywords)\n",
        "corpus = [dictionary.doc2bow(word) for word in keywords]\n",
        "lda = ldamodel.LdaModel(corpus, num_topics=5, random_state=1, iterations=300, passes=10)"
      ],
      "metadata": {
        "id": "_Q88pu8uSa9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "agIMQaAwjXXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models"
      ],
      "metadata": {
        "id": "2oRWQ7z0jj4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary)\n",
        "vis"
      ],
      "metadata": {
        "id": "bRKlmJN3ZXCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn-crfsuite"
      ],
      "metadata": {
        "id": "CjR-6J9TS9-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = load_data('training_data.json')\n",
        "trainer = Trainer(config.load('config.yaml'))\n",
        "trainer.train(training_data)\n",
        "model_directory = trainer.persist('/content/')\n",
        "interpreter = Interpreter.load(model_directory)"
      ],
      "metadata": {
        "id": "0Jdr6xhB0SZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intents = []\n",
        "confidence = []\n",
        "for question in questions:\n",
        "  intents.append(interpreter.parse(question).get('intent').get('name'))\n",
        "  confidence.append(interpreter.parse(question).get('intent').get('confidence'))\n",
        "df['intent'] = intents\n",
        "df['confidence'] = confidence\n",
        "df"
      ],
      "metadata": {
        "id": "cCpCL14au6MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(df[(df.intent == 'coronavirus spread') & (df.confidence >=0.33) & (df.confidence <= 0.43)].index, inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "rkhHz1g1p-f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('questions.csv', sep='\\t', columns=['text'], index=False)"
      ],
      "metadata": {
        "id": "t87Q15sP0pxL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "2de50464451ab144b1ff03cb5eec61b696852d5dd4f5a5c80484648fb6c78eea"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}